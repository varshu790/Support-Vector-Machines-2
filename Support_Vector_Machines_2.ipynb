{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
        "\n",
        "ANS- Polynomial functions and kernel functions are both mathematical tools used in machine learning, especially in algorithms like Support Vector Machines (SVMs) and kernel-based methods.\n",
        "\n",
        "1. **Polynomial Functions:**\n",
        "   - These are functions where the variable is raised to integer powers and multiplied by coefficients. For example, \\(f(x) = ax^2 + bx + c\\) is a quadratic polynomial.\n",
        "   - In the context of machine learning, polynomial functions can be used as basis functions to transform data into higher dimensions, allowing linear algorithms to learn more complex relationships. This process is known as the polynomial feature expansion.\n",
        "\n",
        "2. **Kernel Functions:**\n",
        "   - Kernels are functions that compute the inner product (similarity) between pairs of data points in a transformed space, without explicitly transforming the data.\n",
        "   - Polynomial kernels are a type of kernel function that computes the similarity based on polynomial terms between data points.\n",
        "\n",
        "The relationship between polynomial functions and kernel functions lies in how kernel methods, like SVMs, use polynomial kernels to implicitly perform computations that would otherwise require explicit transformation of data into higher dimensions. Instead of actually computing the transformation of data points into a higher-dimensional space (which can be computationally expensive), kernel functions allow the SVM to work directly in the original space while effectively capturing complex relationships using polynomial terms.\n",
        "\n",
        "In essence, polynomial functions can be employed as the basis for polynomial kernels in kernel methods, enabling these algorithms to efficiently handle non-linear relationships in the original feature space without explicitly transforming the data."
      ],
      "metadata": {
        "id": "2ZO1cGB5n9CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
        "\n",
        "ANS -"
      ],
      "metadata": {
        "id": "XSm9NjZfoFPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD-wk4iwn7l7",
        "outputId": "40c57926-0295-42d9-8ffd-bc95945afc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       0.90      1.00      0.95         9\n",
            "           2       1.00      0.91      0.95        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load an example dataset (e.g., the iris dataset)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Create an SVM classifier with a polynomial kernel\n",
        "# You can specify the degree of the polynomial using the 'degree' parameter\n",
        "svm_classifier = SVC(kernel='poly', degree=3)  # 'degree' specifies the degree of the polynomial kernel\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
        "\n",
        "ANS- In Support Vector Regression (SVR), the epsilon parameter (\\(\\varepsilon\\)) determines the margin of tolerance in the regression model. It defines the width of the epsilon-insensitive tube within which no penalty is associated with errors. Any prediction within this tube is considered accurate and does not contribute to the loss function.\n",
        "\n",
        "The relationship between the value of epsilon and the number of support vectors in SVR is generally inversely proportional:\n",
        "\n",
        "- **Increasing Epsilon:** A larger epsilon allows for a wider margin of tolerance for errors. Consequently, data points can reside farther from the regression line (within the epsilon-insensitive tube) without contributing to the loss. As a result, the SVR model becomes less sensitive to individual data points, potentially leading to fewer support vectors. This is because a wider tube allows more data points to fall within the margin without affecting the model's performance, reducing the need for additional support vectors.\n",
        "\n",
        "- **Decreasing Epsilon:** Conversely, reducing epsilon tightens the margin of tolerance, making the SVR model more sensitive to deviations or errors. With a smaller epsilon, fewer data points are allowed within the margin, resulting in a stricter model that might require more support vectors to accurately capture the relationship between the features and the target variable.\n",
        "\n",
        "However, the exact impact of epsilon on the number of support vectors can vary based on the dataset, the complexity of the problem, and the interplay between epsilon and other hyperparameters in the SVR algorithm (such as the regularization parameter and kernel choice). In practice, tuning epsilon is part of hyperparameter optimization to find the best balance between model complexity and generalization for a given regression task."
      ],
      "metadata": {
        "id": "Fckk_8bNonVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
        "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
        "and provide examples of when you might want to increase or decrease its value?\n",
        "\n",
        "ANS- Absolutely, the performance of Support Vector Regression (SVR) is heavily influenced by various parameters. Let's break down the impact of each parameter:\n",
        "\n",
        "### 1. **Kernel Function:**\n",
        "- **Explanation:** Kernels define the type of transformation used to map the input data into a higher-dimensional space. Common kernels include linear, polynomial, radial basis function (RBF/Gaussian), and sigmoid.\n",
        "- **When to choose:**\n",
        "  - Linear kernels work well when the relationship between features and target is assumed to be linear.\n",
        "  - Polynomial kernels can capture more complex relationships but are sensitive to the choice of degree.\n",
        "  - RBF kernels are versatile and can capture non-linear relationships effectively but might be prone to overfitting if not tuned properly.\n",
        "  - Sigmoid kernels are less commonly used but might be suitable for specific problem types.\n",
        "\n",
        "### 2. **C Parameter (Regularization):**\n",
        "- **Explanation:** C controls the trade-off between maximizing the margin and minimizing the error. A smaller C value allows more margin violations (soft margin), while a larger C value penalizes errors more heavily (hard margin).\n",
        "- **When to increase or decrease:**\n",
        "  - Increase C when you suspect that the model might be underfitting (too much tolerance for errors).\n",
        "  - Decrease C when the model shows signs of overfitting (too much emphasis on individual data points).\n",
        "\n",
        "### 3. **Epsilon Parameter:**\n",
        "- **Explanation:** Epsilon determines the width of the epsilon-insensitive tube. It specifies the margin within which no penalty is associated with errors.\n",
        "- **When to increase or decrease:**\n",
        "  - Increase epsilon to allow larger deviations to be ignored and to create a wider tolerance for errors.\n",
        "  - Decrease epsilon for a tighter tolerance and increased sensitivity to deviations.\n",
        "\n",
        "### 4. **Gamma Parameter (for RBF kernel):**\n",
        "- **Explanation:** Gamma defines the influence range of a single training example. A smaller gamma makes the influence of points more widespread, while a larger gamma makes the influence more localized.\n",
        "- **When to increase or decrease:**\n",
        "  - Increase gamma when the model is underfitting and needs a more complex, localized decision boundary.\n",
        "  - Decrease gamma to prevent overfitting and to create a smoother decision boundary.\n",
        "\n",
        "### Examples:\n",
        "- **High C:** Use when you want to prioritize training accuracy and are willing to tolerate fewer margin violations.\n",
        "- **Low C:** Use when you want to prioritize a wider margin and are willing to tolerate more margin violations.\n",
        "- **High Epsilon:** Suitable when the problem allows for larger prediction errors and you want to accommodate a wider margin of tolerance.\n",
        "- **Low Epsilon:** Useful when precise predictions are crucial, and you want to minimize the tolerance for errors.\n",
        "- **High Gamma:** Suitable for more complex, non-linear relationships and when the data is densely packed.\n",
        "- **Low Gamma:** Useful for simpler, smoother decision boundaries and to avoid overfitting on sparse data.\n",
        "\n",
        "Tuning these parameters often involves experimentation and validation on a holdout set or via cross-validation to find the combination that yields the best performance for a specific problem."
      ],
      "metadata": {
        "id": "9syDo0Oeoust"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Assignment:\n",
        "- Import the necessary libraries and load the dataseg\n",
        "- Split the dataset into training and testing setZ\n",
        "- Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
        "- Create an instance of the SVC classifier and train it on the training datW\n",
        "- hse the trained classifier to predict the labels of the testing datW\n",
        "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
        "precision, recall, F1-scoreK\n",
        "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
        "improve its performanc_\n",
        "- Train the tuned classifier on the entire dataseg\n",
        "- Save the trained classifier to a file for future use."
      ],
      "metadata": {
        "id": "BuEkvW1Vo6vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize features by scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Create an SVC classifier\n",
        "svc = SVC()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for the testing set\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Define a grid of hyperparameters to search through\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf', 'poly']}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Retrieve the best model after hyperparameter tuning\n",
        "best_svc = grid_search.best_estimator_\n",
        "\n",
        "# Train the tuned classifier on the entire dataset\n",
        "best_svc.fit(X, y)\n",
        "\n",
        "# Save the trained classifier to a file using joblib\n",
        "joblib.dump(best_svc, 'trained_svc_classifier.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrV5GECiommQ",
        "outputId": "0c4e28c8-5d1e-4536-ec57-27fb6198f8de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Best Parameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trained_svc_classifier.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}